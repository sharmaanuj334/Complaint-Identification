{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5081,"status":"ok","timestamp":1684700121215,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"O2bPVSPokXpd","outputId":"d5d75112-4e78-4ebb-8819-6804698fecc8"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import torch\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","print(device)"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1055,"status":"ok","timestamp":1684700122262,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"9Yw2Qc3tkalj"},"outputs":[],"source":["import torch\n","import pandas as pd\n","import numpy as np\n","#hello"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16395,"status":"ok","timestamp":1684700138646,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"Z1tPm-BGkcoS","outputId":"974ce20c-e2de-4039-f118-38cb94e5c2b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"]}],"source":["!pip install transformers "]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1684700138647,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"QPbpFGyIkeLK"},"outputs":[],"source":["df = pd.read_csv('/content/Complaint data annotation (explain)_updated - cd (1).csv')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1684700138647,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"Vs8yd-Hgku7V","outputId":"8d5311e5-f7b1-4a20-81e0-1b31cae1b988"},"outputs":[{"data":{"text/plain":["Index(['id', 'tweet', 'label', 'domain', 'sentiment', 'emotion', 'Severity',\n","       'Explain'],\n","      dtype='object')"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["df.keys()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":708,"status":"ok","timestamp":1684700139343,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"-sMinYbXkyFr","outputId":"77bed3f9-4a65-4d0e-ff47-2277669fb01f"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-45e1691c-043b-4315-8fac-474ab0ad36a1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>tweet</th>\n","      <th>label</th>\n","      <th>domain</th>\n","      <th>sentiment</th>\n","      <th>emotion</th>\n","      <th>Severity</th>\n","      <th>Explain</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>@FC_HELP can I return online purchases to a Ho...</td>\n","      <td>0</td>\n","      <td>apparel</td>\n","      <td>Neutral</td>\n","      <td>other</td>\n","      <td>0</td>\n","      <td>can I return online purchases to a House of Fr...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>@FC_Help Hi - I'm writing a piece for MSN Him ...</td>\n","      <td>0</td>\n","      <td>apparel</td>\n","      <td>Positive</td>\n","      <td>other</td>\n","      <td>0</td>\n","      <td>Hi - I'm writing a piece for MSN Him and wonde...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>@FC_Help   i need to check my order</td>\n","      <td>0</td>\n","      <td>apparel</td>\n","      <td>Neutral</td>\n","      <td>other</td>\n","      <td>0</td>\n","      <td>i need to check my order</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>@FC_Help I need to get in contact with someone...</td>\n","      <td>1</td>\n","      <td>apparel</td>\n","      <td>Neutral</td>\n","      <td>other</td>\n","      <td>1</td>\n","      <td>I need to get in contact with someone regardin...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>@FC_Help How can I get a hold of you so we can...</td>\n","      <td>0</td>\n","      <td>apparel</td>\n","      <td>Negative</td>\n","      <td>other</td>\n","      <td>0</td>\n","      <td>How can I get a hold of you so we can discuss ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-45e1691c-043b-4315-8fac-474ab0ad36a1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-45e1691c-043b-4315-8fac-474ab0ad36a1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-45e1691c-043b-4315-8fac-474ab0ad36a1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   id                                              tweet  label   domain  \\\n","0   1  @FC_HELP can I return online purchases to a Ho...      0  apparel   \n","1   2  @FC_Help Hi - I'm writing a piece for MSN Him ...      0  apparel   \n","2   3                @FC_Help   i need to check my order      0  apparel   \n","3   4  @FC_Help I need to get in contact with someone...      1  apparel   \n","4   5  @FC_Help How can I get a hold of you so we can...      0  apparel   \n","\n","  sentiment emotion  Severity  \\\n","0   Neutral   other         0   \n","1  Positive   other         0   \n","2   Neutral   other         0   \n","3   Neutral   other         1   \n","4  Negative   other         0   \n","\n","                                             Explain  \n","0  can I return online purchases to a House of Fr...  \n","1  Hi - I'm writing a piece for MSN Him and wonde...  \n","2                           i need to check my order  \n","3  I need to get in contact with someone regardin...  \n","4  How can I get a hold of you so we can discuss ...  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1684700139344,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"2foIDP1gmXDF"},"outputs":[],"source":["domain_dict = {\n","    \"other\" : 0,\n","    \"services\" : 1,\n","    \"random_reply\" : 2,\n","    \"software\" : 3,\n","    \"retail\" : 4,\n","    \"random_tweet\" : 5,\n","    \"transport\" : 6,\n","    \"cars\" : 7,\n","    \"food\" : 8,\n","    \"apparel\" : 9,\n","    \"electronics\" : 10\n","}\n","\n","# note to take string lower\n","senti_dict = {\n","    'negative' : 0,\n","    'positive' : 1,\n","    'neutral' : 2\n","}\n","\n","emo_dict = {\n","    'sadness' : 0, \n","    'joy' : 1, \n","    'other' : 2, \n","    'anger' : 3, \n","    'disgust' : 4, \n","    'surprise' : 5, \n","    'fear' : 6\n","}"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1684700139345,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"CVP51OEwnMzu"},"outputs":[],"source":["label = []\n","domain = []\n","emotion = []\n","sentiment = []\n","severity = []\n","explain = []\n","\n","for i in range(len(df)):\n","  if (pd.isna(df['label'][i]) or pd.isna(df['domain'][i]) or pd.isna(df['emotion'][i]) or pd.isna(df['sentiment'][i]) or pd.isna(df['Severity'][i]) or pd.isna(df['Explain'][i])):\n","    continue\n","  label.append(df['label'][i])\n","  domain.append(domain_dict[df['domain'][i]])\n","  emotion.append(emo_dict[df['emotion'][i]])\n","  sentiment.append(senti_dict[(df['sentiment'][i]).lower()])\n","  severity.append(df['Severity'][i])\n","  explain.append(df['Explain'][i])"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":548,"status":"ok","timestamp":1684700139871,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"8m6kdXgPqCmj"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","label_train, label_test, domain_train, domain_test, emotion_train, emotion_test, sentiment_train, sentiment_test, severity_train, severity_test, explain_train, explain_test = train_test_split(label, domain, emotion, sentiment, severity, explain, test_size = 0.2, random_state = 42, shuffle = True)\n","label_train, label_val, domain_train, domain_val, emotion_train, emotion_val, sentiment_train, sentiment_val, severity_train, severity_val, explain_train, explain_val = train_test_split(label_train, domain_train, emotion_train, sentiment_train, severity_train, explain_train, test_size = 0.2, random_state = 42, shuffle = True)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["1fe3b26fd70d40b6a7958d5b5cc3f486","72f460aaf8054155a659b76267a0b617","9469337726894692a7dcebc8fd95191d","c3e32ec92ac1402e8065d621d7fca871","4b98ba4a541b4008b46a4d577144446f","4a65addb4f9f479083435861dca8d776","554c916927b44e6082bb550b7ed0c4bb","ff42a75457384ebe90229ca171f24a59","f5973f4a373a45e6b9c0273de38291f1","8086c58f1e894b808eac8d66eaa0653a","6bce2f771fb64bd0bca113f0a955d24b","e97aee74212243359d226fcd61d258e3","016e3578fe7343d28d130f02b415e449","d0f203efd4b143eea2d3c133a16c4598","2b890c5a78c24dc29a3d979a3d86b8cc","70417d99b6d849cf8a7263f92a059c28","9afd2a789d1640f591f0935d3609d012","aace6f9220114616a41d537ffc3153da","715c541430ef46f594a1dce7dbd1b563","f0b373e04199497ebf8618ce16a11b11","fa4d55127d5242649d8e6fab90808dbc","897581ef390747819af90caecebccc76","cf063c23004c4d56b9aa0e996fd787c7","737bd17f81614d22ac623b3cd13cd679","340f1d89c22043629093b13ceb8a8fc3","b2fc7e9d13364523a433c27ee89c14db","9537617131c0451e907749bdab06f521","6bfc8b04ffa84568aeef1c8da7bb9730","be1bbea3a3934668b2bb0baf041acdf7","73d2196e952440d3943d2be21dfd24d4","55ab4f485b7f414ebea7d51b56b5bd40","908b634aa0e944b49deae3b8896b0ec2","5bbbce3e7cea483c90fe3982f3f8b7b0"]},"executionInfo":{"elapsed":1632,"status":"ok","timestamp":1684700141497,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"f0w5ajsPrGSm","outputId":"cf5ed149-fcdc-4fe8-99db-7ce2e5b0d958"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1fe3b26fd70d40b6a7958d5b5cc3f486","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e97aee74212243359d226fcd61d258e3","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cf063c23004c4d56b9aa0e996fd787c7","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import BertTokenizer\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","def preprocess_data(text):\n","  g_input_ids = []\n","  g_attention_mask = []\n","\n","  for line in text:\n","    inputs = tokenizer(line, return_tensors=\"pt\")\n","\n","    input_ids = inputs[\"input_ids\"]\n","    attention_mask = inputs[\"attention_mask\"]\n","\n","    size = input_ids.shape[1]\n","\n","    if size < 60:\n","      input_ids = torch.cat((input_ids, torch.zeros(1, 60-size)), dim = 1)\n","      attention_mask = torch.cat((attention_mask, torch.zeros(1, 60-size)), dim = 1)\n","    else:\n","      input_ids = input_ids[:, :60]\n","      attention_mask = attention_mask[:, :60]\n","    \n","    # Why we use np.array()\n","    g_input_ids.append(np.array(input_ids.reshape(-1)))\n","    g_attention_mask.append(np.array(attention_mask.reshape(-1)))\n","\n","  g_input_ids = torch.tensor(g_input_ids)\n","  g_attention_mask = torch.tensor(g_attention_mask)\n","\n","  return g_input_ids, g_attention_mask"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1818,"status":"ok","timestamp":1684700143311,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"G87hIDIdyZa1","outputId":"54cd2a23-e28c-461f-f8aa-9e29b94c28ed"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-10-eb1c066b164e>:28: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n","  g_input_ids = torch.tensor(g_input_ids)\n"]}],"source":["input_ids_train, attention_mask_train = preprocess_data(explain_train)\n","input_ids_val, attention_mask_val = preprocess_data(explain_val)\n","input_ids_test, attention_mask_test = preprocess_data(explain_test)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1684700143312,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"bBQt0Q6iyaHl"},"outputs":[],"source":["class Complaint_dataset():\n","  def __init__(self, input_ids, attention_mask, label, domain, emotion, sentiment, severity):\n","    self.input_ids = input_ids\n","    self.attention_mask = attention_mask\n","    self.label = label\n","    self.domain = domain\n","    self.emotion = emotion\n","    self.sentiment = sentiment\n","    self.severity = severity\n","  def __len__(self):\n","    return len(self.input_ids)\n","  def __getitem__(self, idx):\n","    if torch.is_tensor(idx):\n","      idx = idx.tolist()\n","    \n","    sample = {\n","        \"input_ids\" : torch.tensor(self.input_ids[idx]).long().to(device),\n","        \"attention_mask\" : torch.tensor(self.attention_mask[idx]).long().to(device),\n","        \"label\" : torch.tensor(self.label[idx]).long().to(device),\n","        \"domain\" : torch.tensor(self.domain[idx]).long().to(device),\n","        \"emotion\" : torch.tensor(self.emotion[idx]).long().to(device),\n","        \"sentiment\" : torch.tensor(self.sentiment[idx]).long().to(device),\n","        \"severity\" : torch.tensor(self.severity[idx]).long().to(device)\n","    }\n","\n","    return sample"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1684700143313,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"vbPbUUxNymMu"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","complaint_train = Complaint_dataset(input_ids_train, attention_mask_train, label_train, domain_train, emotion_train, sentiment_train, severity_train)\n","train_dataloader = DataLoader(complaint_train, batch_size = 32, shuffle = False)\n","\n","complaint_val = Complaint_dataset(input_ids_val, attention_mask_val, label_val, domain_val, emotion_val, sentiment_val, severity_val)\n","val_dataloader = DataLoader(complaint_val, batch_size = 32, shuffle = False)\n","\n","complaint_test = Complaint_dataset(input_ids_test, attention_mask_test, label_test, domain_test, emotion_test, sentiment_test, severity_test)\n","test_dataloader = DataLoader(complaint_test, batch_size = 32, shuffle = False)"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1684700143313,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"DknUGhh90DCl"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import BertTokenizer, BertModel\n","\n","# What is super(Complaint) doing and why the name __init__() as such\n","# Practise OOPS\n","\n","class Complaint(nn.Module):\n","  def __init__(self):\n","    super(Complaint, self).__init__()\n","    self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n","    self.complaint = nn.Linear(768, 2)\n","    self.severity = nn.Linear(768, 5)\n","  def forward(self, input_ids, attention_mask):\n","    data = self.bert_model(input_ids = input_ids, attention_mask = attention_mask).pooler_output\n","    Complaint = self.complaint(data)\n","    Severity = self.severity(data)\n","    return Complaint, Severity"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":884,"referenced_widgets":["2e048e482e9345808f348ce8bfd9f7a7","051534d1b4274ec381e37b9c178fbe5f","0b5c58df461545a093c2feb987367fdf","1619eede37d64d36996ef1044ede5bf2","e87d512953414cf8aa96850432fb46f6","1b5b26753f7f472d8e512a8f10f2abca","ca30e66617d94c518e9aa5bc132e3467","8344248e0c0145e7819fa213e81a3e69","251ce3f8b49b4e238b98c183a3792569","e31ae7bcb67e47fbb6d8a5adc489c4d5","e198798a62284569b1035f4d77d800b3"]},"executionInfo":{"elapsed":3227020,"status":"ok","timestamp":1684703370327,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"wkXWdsQY7Qm2","outputId":"3dba4203-bf36-469c-a81e-6b3ad5f75ef7"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2e048e482e9345808f348ce8bfd9f7a7","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","<ipython-input-12-aa0febf124e5>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \"input_ids\" : torch.tensor(self.input_ids[idx]).long().to(device),\n","<ipython-input-12-aa0febf124e5>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \"attention_mask\" : torch.tensor(self.attention_mask[idx]).long().to(device),\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 1\n","Validation loss : 3.2234461307525635\n","Epoch : 2\n","Validation loss : 2.836848482489586\n","Epoch : 3\n","Validation loss : 2.588334619998932\n","Epoch : 4\n","Validation loss : 2.6125908121466637\n","Epoch : 5\n","Validation loss : 2.75778342038393\n","Epoch : 6\n","Validation loss : 3.2326973900198936\n","Epoch : 7\n","Validation loss : 2.814572587609291\n","Epoch : 8\n","Validation loss : 2.932586297392845\n","Epoch : 9\n","Validation loss : 2.8130855336785316\n","Epoch : 10\n","Validation loss : 3.0867473110556602\n","Epoch : 11\n","Validation loss : 2.762418657541275\n","Epoch : 12\n","Validation loss : 2.9909066781401634\n","Epoch : 13\n","Validation loss : 2.775322027504444\n","Epoch : 14\n","Validation loss : 3.188516542315483\n","Epoch : 15\n","Validation loss : 2.81193508207798\n","Epoch : 16\n","Validation loss : 2.9587571173906326\n","Epoch : 17\n","Validation loss : 2.8014901280403137\n","Epoch : 18\n","Validation loss : 3.343397356569767\n","Epoch : 19\n","Validation loss : 2.846118301153183\n","Epoch : 20\n","Validation loss : 2.9938517808914185\n"]}],"source":["complaint_model = Complaint()\n","complaint_model = complaint_model.to(device)\n","complaint_model.train()\n","\n","loss_func = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(complaint_model.parameters(), lr = 1e-5, weight_decay = 0)\n","epochs = 20\n","min_val_loss = 10\n","\n","complaint_model.train()\n","\n","for i in range(epochs):\n","\n","  for data in train_dataloader:\n","    complaint_model.zero_grad()\n","    comp_train = data[\"label\"].to(device)\n","    sev_train = data[\"severity\"].to(device)\n","    emo_train = data[\"emotion\"].to(device)\n","    senti_train = data[\"sentiment\"].to(device)\n","    comp_out, sev_out = complaint_model.forward(input_ids = data[\"input_ids\"].to(device), attention_mask = data[\"attention_mask\"].to(device))\n","\n","    loss_train = loss_func(comp_out, comp_train) + loss_func(sev_out, sev_train)\n","    loss_train.backward()\n","    optimizer.step()\n","\n","    complaint_model.eval()\n","\n","    with torch.no_grad():\n","      total_loss_val = 0\n","      for data in val_dataloader:\n","        comp_val = data[\"label\"].to(device)\n","        sev_val = data[\"severity\"].to(device)\n","        emo_val = data[\"emotion\"].to(device)\n","        senti_val = data[\"sentiment\"].to(device)\n","        comp_val_out, sev_val_out = complaint_model(input_ids = data[\"input_ids\"].to(device), attention_mask = data[\"attention_mask\"].to(device))\n","\n","        val_loss = loss_func(comp_val_out, comp_val) + loss_func(sev_val_out, sev_val)\n","\n","        total_val = comp_val.size(0)\n","        total_loss_val += val_loss.item()\n","\n","      val_loss = total_loss_val / total_val\n","\n","    if ((min_val_loss-val_loss) > 1e-4):\n","      min_val_loss = val_loss\n","      torch.save(complaint_model.state_dict(), \"bert_model.pt\")\n","\n","  print(f\"Epoch : {i+1}\")\n","  print(f\"Validation loss : {val_loss}\")\n","\n","  complaint_model.load_state_dict(torch.load(\"bert_model.pt\"))"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1684703370329,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"h1NLvSBHHVvP"},"outputs":[],"source":["from sklearn.metrics import *"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5430,"status":"ok","timestamp":1684703375739,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"-vcgpzrUNKQ9","outputId":"903920c2-b437-4d25-cf67-695e72515b92"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchmetrics\n","  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/519.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m419.8/519.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n","Installing collected packages: torchmetrics\n","Successfully installed torchmetrics-0.11.4\n"]}],"source":["!pip install torchmetrics"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2978,"status":"ok","timestamp":1684703378691,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"Mnt4YzqaUxgC","outputId":"7c1b3865-8cd8-4cfa-bbaf-6e7fc25ec212"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-12-aa0febf124e5>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \"input_ids\" : torch.tensor(self.input_ids[idx]).long().to(device),\n","<ipython-input-12-aa0febf124e5>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \"attention_mask\" : torch.tensor(self.attention_mask[idx]).long().to(device),\n"]},{"name":"stdout","output_type":"stream","text":["comp_test_accuracy : 0.8681159615516663\n","comp_test_precision : 0.7941176295280457\n","comp_test_f1 : 0.8260038495063782\n","comp_test_recall : 0.8605577945709229\n","sev_test_accuracy : 0.3460594713687897\n","sev_test_precision : 0.2982306480407715\n","sev_test_f1 : 0.31934285163879395\n","sev_test_recall : 0.3460594713687897\n"]}],"source":["from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryF1Score, BinaryRecall\n","from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassF1Score, MulticlassRecall\n","\n","comp_accuracy = BinaryAccuracy()\n","comp_precision = BinaryPrecision()\n","comp_f1 = BinaryF1Score()\n","comp_recall = BinaryRecall() \n","\n","sev_accuracy = MulticlassAccuracy(num_classes=5)\n","sev_precision = MulticlassPrecision(num_classes=5)\n","sev_f1 = MulticlassF1Score(num_classes=5)\n","sev_recall = MulticlassRecall(num_classes=5)\n","\n","with torch.no_grad():\n","    for data in test_dataloader:\n","      lab_comp = data[\"label\"]\n","      lab_sev = data[\"severity\"]\n","\n","      comp_out, sev_out = complaint_model.forward(input_ids=data[\"input_ids\"], attention_mask=data[\"attention_mask\"])\n","\n","      _, pred_comp = torch.max(comp_out.data,1)\n","      _, pred_sev = torch.max(sev_out.data,1)\n","\n","      comp_accuracy.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_precision.update(pred_comp.cpu(), lab_comp.cpu()) \n","      comp_f1.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_recall.update(pred_comp.cpu(), lab_comp.cpu())\n","\n","      sev_accuracy.update(pred_sev.cpu(), lab_sev.cpu())\n","      sev_precision.update(pred_sev.cpu(), lab_sev.cpu())\n","      sev_f1.update(pred_sev.cpu(), lab_sev.cpu())\n","      sev_recall.update(pred_sev.cpu(), lab_sev.cpu())\n","\n","    comp_test_accuracy = comp_accuracy.compute()\n","    comp_test_precision = comp_precision.compute()\n","    comp_test_f1 = comp_f1.compute()\n","    comp_test_recall = comp_recall.compute() \n","\n","    sev_test_accuracy = sev_accuracy.compute()\n","    sev_test_precision = sev_precision.compute()\n","    sev_test_f1 = sev_f1.compute()\n","    sev_test_recall = sev_recall.compute()\n","\n","    print(f\"comp_test_accuracy : {comp_test_accuracy}\")\n","    print(f\"comp_test_precision : {comp_test_precision}\")\n","    print(f\"comp_test_f1 : {comp_test_f1}\")\n","    print(f\"comp_test_recall : {comp_test_recall}\") \n","\n","    print(f\"sev_test_accuracy : {sev_test_accuracy}\")\n","    print(f\"sev_test_precision : {sev_test_precision}\")\n","    print(f\"sev_test_f1 : {sev_test_f1}\")\n","    print(f\"sev_test_recall : {sev_test_recall}\") "]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":404,"status":"ok","timestamp":1684704460083,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"-rKbLUZwMfwf"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer, AutoModel\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684704461802,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"aze89UbaMfsP"},"outputs":[],"source":["embed_input = \"bert\"\n","bitmask_input = \"comp+sent\"\n","hid_size = 150"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":430,"status":"ok","timestamp":1684704510870,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"h_nZEx-hMfpU"},"outputs":[],"source":["bitmasks = {\n","    \"comp\" : 1,\n","    \"sent\" : 2,\n","    \"emo\" : 4,\n","    \"comp+sent\" : 3,\n","    \"comp+emo\" : 5,\n","    \"comp+sev\" : 9,\n","    \"comp+sent+emo\" : 7,\n","    \"comp+sent+sev\" : 11,\n","    \"comp+emo+sev\" : 13,\n","    \"comp+emo+sev+sent\" : 15\n","}\n","\n","embeds = {\n","    \"bert\" : \"bert-base-uncased\",\n","    \"roberta\" : \"roberta-base\"\n","}"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":400,"status":"ok","timestamp":1684704513027,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"V9vEvHBmMfnc"},"outputs":[],"source":["learning_rate = 1e-5\n","epochs = 20\n","min_val_loss = 10"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1684704515451,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"3Ltb0mTqMfjr"},"outputs":[],"source":["class complaint(nn.Module):\n","  def __init__(self, hidden_size, bitmask, embedding):\n","    super(complaint, self).__init__()\n","    self.emb_model = AutoModel.from_pretrained(embedding)\n","    self.dropout = nn.Dropout(p = 0.5)\n","    self.hidden = nn.Linear(768, hidden_size)\n","    self.comp = nn.Linear(hidden_size, 2) #Complaint Output_Size = 2\n","    self.sent = nn.Linear(hidden_size, 3) #Sentiment Output_Size = 3\n","    self.sev = nn.Linear(hidden_size, 5) #Severity Level Output_Size = 5\n","    self.emo = nn.Linear(hidden_size, 7) #Emotional Level Output_Size = 7\n","    self.bitmask = bitmask\n","\n","  def forward_comp(self, x):\n","    out = self.comp(x)\n","    return out\n","  \n","  def forward_sent(self, x):\n","    out = self.sent(x)\n","    return out\n","  \n","  def forward_sev(self, x):\n","    out = self.sev(x)\n","    return out\n","  \n","  def forward_emo(self, x):\n","    out = self.emo(x)\n","    return out\n","\n","  def forward(self, input_ids, attention_mask, bitmask):\n","    data = self.dropout(self.emb_model(input_ids=input_ids, attention_mask=attention_mask).pooler_output)\n","    out = self.hidden(data)\n","    if (bitmask == bitmasks[\"comp\"]):\n","      out = self.forward_comp(out)\n","      return out\n","    \n","    if (bitmask == bitmasks[\"sent\"]):\n","      out = self.forward_sent(out)\n","      return out\n","    \n","    if (bitmask == bitmasks[\"emo\"]):\n","      out = self.forward_emo(out)\n","      return out\n","\n","    elif (bitmask == bitmasks[\"comp+sent\"]):\n","      out1 = self.forward_comp(out)\n","      out2 = self.forward_sent(out)\n","      return out1, out2\n","    elif(bitmask == bitmasks[\"comp+emo\"]):\n","      out1 = self.forward_comp(out)\n","      out2 = self.forward_emo(out)\n","      return out1, out2\n","    elif(bitmask == bitmasks[\"comp+sev\"]):\n","      out1 = self.forward_comp(out)\n","      out2 = self.forward_sev(out)\n","      return out1, out2\n","    elif(bitmask == bitmasks[\"comp+sent+emo\"]):\n","      out1 = self.forward_comp(out)\n","      out2 = self.forward_sent(out)\n","      out3 = self.forward_emo(out)\n","      return out1, out2, out3\n","    elif(bitmask == bitmasks[\"comp+sent+sev\"]):\n","      out1 = self.forward_comp(out)\n","      out2 = self.forward_sent(out)\n","      out3 = self.forward_sev(out)\n","      return out1, out2, out3\n","    elif(bitmask == bitmasks[\"comp+emo+sev\"]):\n","      out1 = self.forward_comp(out)\n","      out2 = self.forward_emo(out)\n","      out3 = self.forward_sev(out)\n","      return out1, out2, out3\n","    elif(bitmask == bitmasks[\"comp+emo+sev+sent\"]):\n","      out1 = self.forward_comp(out)\n","      out2 = self.forward_emo(out)\n","      out3 = self.forward_sev(out)\n","      out4 = self.forward_sent(out)\n","      return out1, out2, out3, out4"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1728,"status":"ok","timestamp":1684704520915,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"Vcb4fY3xMfgt","outputId":"f5a3de0e-5e6a-43bb-e34a-e817f44d6a68"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/openpyxl/worksheet/_reader.py:312: UserWarning: Unknown extension is not supported and will be removed\n","  warn(msg)\n"]}],"source":["df = pd.read_excel(\"/content/aspect_complain_formatted.xlsx\", \"new-FINCORP\")"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1684704520915,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"NvEdFrFlMfdV"},"outputs":[],"source":["doc = []\n","label = []\n","\n","label_number = {\n","    \"comp\" : 0,\n","    \"sent\" : 1,\n","    \"sev\" : 2,\n","    \"emo\" : 3\n","}\n","\n","sent_num = {\n","    \"positive\" : 2,\n","    \"neutral\" : 1,\n","    \"negative\" : 0\n","}\n","\n","sev_num = {\n","    \"accusation\" : 0,\n","    \"blame\" : 1,\n","    \"disapproval\" : 2,\n","    \"no explicit reproach\" : 3,\n","    \"non-complaint\" : 4\n","}\n","\n","emo_num = {\n","    \"anger\" : 0,\n","    \"disgust\" : 1,\n","    \"fear\" : 2,\n","    \"happiness\" : 3,\n","    \"other\" : 4,\n","    \"sadness\" : 5,\n","    \"surprise\" : 6\n","}\n","\n","for i in range(len(df)):\n","  doc.append((df[\"Complaint/ Opinion\"])[i])\n","  temp = []\n","  temp.append((df[\"Over-all_Complaint Label\"])[i])\n","  temp.append(sent_num[((df[\"Sentiment\"])[i]).lower()])\n","  temp.append(sev_num[((df[\"Severity level\"])[i]).lower()])\n","  temp.append(emo_num[((df[\"Emotion\"])[i]).lower()])\n","  label.append(temp)\n","\n","doc_tr, doc_test, label_tr, label_test = train_test_split(doc, label, test_size=0.2, random_state = 42, shuffle=True)\n","doc_train, doc_val, label_train, label_val = train_test_split(doc_tr,label_tr, test_size=0.125, random_state = 42, shuffle=True)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["32ed8b1c877d4ed39362bb3464d9eff7","1b201b5ca7b24378bfc6f7bb64e1bbf0","84c9ab5130a441ecacd8a653bcf03ed4","0f5f8909eb5e420898d1894171383400","7ef73da28f6047b08e988a58e8da3b3b","6fceb4737505411bb57609076cc564c4","1cae7f493e004b1696998c3a755b144e","30cf8e8dac834af1b9844d67485ab2fc","36dc54fc4c494e8fbf52773e3b87fcf1","8eb3550559df47c1a32067a0cfc847a4","0d87f2f5148e4ebe8a51b4d8094e2583"]},"executionInfo":{"elapsed":711,"status":"ok","timestamp":1684704524118,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"j78yVWd2Mfao","outputId":"b7ce0a17-64a7-45c9-f42f-940271818689"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"32ed8b1c877d4ed39362bb3464d9eff7","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = AutoTokenizer.from_pretrained(embeds[embed_input])"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684704527206,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"YtUXyT2pMfXm"},"outputs":[],"source":["def preprocess_data(doc):\n","  \n","  g_input_ids = []\n","  g_attention_mask = []\n","  \n","  for x in doc:\n","    inputs = tokenizer(x, return_tensors=\"pt\")\n","\n","    input_ids = inputs[\"input_ids\"]\n","    attention_mask = inputs[\"attention_mask\"]\n","\n","    size = input_ids.shape[1]\n","\n","    if size < 60:\n","      input_ids = torch.cat((input_ids, torch.zeros(1, 60 - size)), dim=1)\n","      attention_mask = torch.cat((attention_mask, torch.zeros(1, 60 - size)), dim=1)\n","    else:\n","      input_ids = input_ids[:, :60]\n","      attention_mask = attention_mask[:, :60]\n","    \n","    g_input_ids.append(np.array(input_ids.reshape(-1)))\n","    g_attention_mask.append(np.array(attention_mask.reshape(-1)))\n","  \n","  g_input_ids = torch.tensor(g_input_ids)\n","  g_attention_mask = torch.tensor(g_attention_mask)\n","\n","  return g_input_ids, g_attention_mask"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":2636,"status":"ok","timestamp":1684704532303,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"4wawgKXFMfUw"},"outputs":[],"source":["input_ids_train,attention_mask_train = preprocess_data(doc_train)\n","input_ids_val, attention_mask_val = preprocess_data(doc_val)\n","input_ids_test, attention_mask_test = preprocess_data(doc_test)"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1684704532303,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"BUi40NJ5MfRo"},"outputs":[],"source":["class complaint_dataset(torch.utils.data.Dataset):\n","  def __init__(self, g_input_ids, g_attention_mask, label_comp, label_sent, label_sev, label_emo):\n","    self.input_ids = g_input_ids\n","    self.attention_mask = g_attention_mask\n","    self.label_comp = label_comp\n","    self.label_sent = label_sent\n","    self.label_sev = label_sev\n","    self.label_emo = label_emo\n","  \n","  def __len__(self):\n","    return len(self.label_comp)\n","  \n","  def __getitem__(self, idx):\n","\n","    if torch.is_tensor(idx):\n","      idx = idx.tolist()\n","    \n","    sample = {\n","        # \"input_ids\" : torch.tensor(self.input_ids[idx]).long().to(device),\n","        # \"attention_mask\" : torch.tensor(self.attention_mask[idx]).long().to(device),\n","        # \"label_comp\" : torch.tensor(self.label_comp[idx]).long().to(device),\n","        # \"label_sent\" : torch.tensor(self.label_sent[idx]).long().to(device),\n","        # \"label_sev\" : torch.tensor(self.label_sev[idx]).long().to(device),\n","        # \"label_emo\" : torch.tensor(self.label_emo[idx]).long().to(device)\n","\n","        \"input_ids\" : torch.tensor(self.input_ids[idx]).long(),\n","        \"attention_mask\" : torch.tensor(self.attention_mask[idx]).long(),\n","        \"label_comp\" : torch.tensor(self.label_comp[idx]).long(),\n","        \"label_sent\" : torch.tensor(self.label_sent[idx]).long(),\n","        \"label_sev\" : torch.tensor(self.label_sev[idx]).long(),\n","        \"label_emo\" : torch.tensor(self.label_emo[idx]).long()\n","    }\n","\n","    return sample\n"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684704534441,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"aQYCndyGMfPg"},"outputs":[],"source":["train_comp = []\n","train_sent = []\n","train_sev = []\n","train_emo = []\n","val_comp = []\n","val_sent = []\n","val_sev = []\n","val_emo = []\n","test_comp = []\n","test_sent = []\n","test_sev = []\n","test_emo = []\n","\n","for x in label_train:\n","  train_comp.append(x[0])\n","  train_sent.append(x[1])\n","  train_sev.append(x[2])\n","  train_emo.append(x[3])\n","\n","for x in label_val:\n","  val_comp.append(x[0])\n","  val_sent.append(x[1])\n","  val_sev.append(x[2])\n","  val_emo.append(x[3])\n","\n","for x in label_test:\n","  test_comp.append(x[0])\n","  test_sent.append(x[1])\n","  test_sev.append(x[2])\n","  test_emo.append(x[3])"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":403,"status":"ok","timestamp":1684704538008,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"z9EX4kzTMfLH"},"outputs":[],"source":["complaint_train = complaint_dataset(input_ids_train, attention_mask_train, train_comp, train_sent, train_sev, train_emo)\n","train_dataloader = DataLoader(complaint_train, batch_size = 32, shuffle = False)\n","\n","complaint_val = complaint_dataset(input_ids_val, attention_mask_val, val_comp, val_sent, val_sev, val_emo)\n","val_dataloader = DataLoader(complaint_val, batch_size = 32 ,shuffle = False)\n","\n","complaint_test = complaint_dataset(input_ids_test, attention_mask_test, test_comp, test_sent, test_sev, test_emo)\n","test_dataloader = DataLoader(complaint_test, batch_size = 32, shuffle = False)"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684704539556,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"IXCu1aDCMfHw"},"outputs":[],"source":["class CELoss(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.xent_loss = nn.CrossEntropyLoss()\n","\n","    def forward(self, outputs, targets):\n","        return self.xent_loss(outputs, targets)\n","\n","\n","class SupConLoss(nn.Module):\n","\n","    def __init__(self, alpha, temp):\n","        super().__init__()\n","        self.xent_loss = nn.CrossEntropyLoss()\n","        self.alpha = alpha\n","        self.temp = temp\n","\n","    def nt_xent_loss(self, anchor, target, labels):\n","        with torch.no_grad():\n","            labels = labels.unsqueeze(-1)\n","            mask = torch.eq(labels, labels.transpose(0, 1))\n","            # delete diag elem\n","            mask = mask ^ torch.diag_embed(torch.diag(mask))\n","        # compute logits\n","        anchor_dot_target = torch.einsum('bd,cd->bc', anchor, target) / self.temp\n","        # delete diag elem\n","        anchor_dot_target = anchor_dot_target - torch.diag_embed(torch.diag(anchor_dot_target))\n","        # for numerical stability\n","        logits_max, _ = torch.max(anchor_dot_target, dim=1, keepdim=True)\n","        logits = anchor_dot_target - logits_max.detach()\n","        # compute log prob\n","        exp_logits = torch.exp(logits)\n","        # mask out positives\n","        logits = logits * mask\n","        log_prob = logits - torch.log(exp_logits.sum(dim=1, keepdim=True) + 1e-12)\n","        # in case that mask.sum(1) is zero\n","        mask_sum = mask.sum(dim=1)\n","        mask_sum = torch.where(mask_sum == 0, torch.ones_like(mask_sum), mask_sum)\n","        # compute log-likelihood\n","        pos_logits = (mask * log_prob).sum(dim=1) / mask_sum.detach()\n","        loss = -1 * pos_logits.mean()\n","        return loss\n","\n","    def forward(self, outputs, targets):\n","        ce_loss = (1 - self.alpha) * self.xent_loss(outputs, targets)\n","        cl_loss = self.alpha * self.nt_xent_loss(outputs, outputs, targets)\n","        return ce_loss + cl_loss\n","\n","\n","class DualLoss(SupConLoss):\n","\n","    def __init__(self, alpha, temp):\n","        super().__init__(alpha, temp)\n","\n","    def forward(self, outputs, targets):\n","        normed_cls_feats = F.normalize(outputs['cls_feats'], dim=-1)\n","        normed_label_feats = F.normalize(outputs['label_feats'], dim=-1)\n","        normed_pos_label_feats = torch.gather(normed_label_feats, dim=1, index=targets.unsqueeze(-1).expand(-1, 1, normed_label_feats.size(-1))).squeeze(1)\n","        ce_loss = (1 - self.alpha) * self.xent_loss(outputs['predicts'], targets)\n","        cl_loss_1 = 0.5 * self.alpha * self.nt_xent_loss(normed_pos_label_feats, normed_cls_feats, targets)\n","        cl_loss_2 = 0.5 * self.alpha * self.nt_xent_loss(normed_cls_feats, normed_pos_label_feats, targets)\n","        return ce_loss + cl_loss_1 + cl_loss_2"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":468,"status":"ok","timestamp":1684704543239,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"HbSrFSibMfE0"},"outputs":[],"source":["criterion = SupConLoss(0.1, 0.1)"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1684704544464,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"2d6SBkIPMfB0"},"outputs":[],"source":["from sklearn.metrics import *"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4072,"status":"ok","timestamp":1684704549973,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"hxT-sjgbMe-f","outputId":"f01f408a-85e4-4b13-8d16-bba5456dc600"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (0.11.4)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.22.4)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n"]}],"source":["!pip install torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_-XG8ttfOFvC"},"outputs":[],"source":["complaint_model = complaint(hidden_size = hid_size, bitmask=bitmasks[bitmask_input], embedding=embeds[embed_input])\n","\n","\n","model_name = embed_input + \"_\" + bitmask_input + \".pt\"\n","\n","for i in range(epochs):\n","\n","  total_loss_train = 0\n","  total_train = 0\n","\n"," \n","      \n","\n","  complaint_model.load_state_dict(torch.load(model_name))"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":504},"executionInfo":{"elapsed":22,"status":"error","timestamp":1684705035252,"user":{"displayName":"meachine learning","userId":"02615647607236970242"},"user_tz":-330},"id":"Lq2HIUe6NYl5","outputId":"e06087ae-74b7-4603-a291-e0c470ef0ead"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-29-d649379ce403>:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \"input_ids\" : torch.tensor(self.input_ids[idx]).long(),\n","<ipython-input-29-d649379ce403>:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \"attention_mask\" : torch.tensor(self.attention_mask[idx]).long(),\n"]},{"ename":"RuntimeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-7b47d7ae59f4>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0mlab_sev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label_sev\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mlab_emo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label_emo\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomplaint_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab_comp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-9fbaa8361c5d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseverity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mComplaint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplaint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mSeverity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseverity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m   1014\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2208\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"]}],"source":["from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryF1Score, BinaryRecall\n","from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassF1Score, MulticlassRecall\n","\n","comp_accuracy = BinaryAccuracy()\n","comp_precision = BinaryPrecision()\n","comp_f1 = BinaryF1Score()\n","comp_recall = BinaryRecall()\n","\n","sent_accuracy = MulticlassAccuracy(num_classes=3)\n","sent_precision = MulticlassPrecision(num_classes=3)\n","sent_f1 = MulticlassF1Score(num_classes=3)\n","sent_recall = MulticlassRecall(num_classes=3)\n","\n","sev_accuracy = MulticlassAccuracy(num_classes=5)\n","sev_precision = MulticlassPrecision(num_classes=5)\n","sev_f1 = MulticlassF1Score(num_classes=5)\n","sev_recall = MulticlassRecall(num_classes=5)\n","\n","emo_accuracy = MulticlassAccuracy(num_classes=7)\n","emo_precision = MulticlassPrecision(num_classes=7)\n","emo_f1 = MulticlassF1Score(num_classes=7)\n","emo_recall = MulticlassRecall(num_classes=7)\n","\n","with torch.no_grad():\n","  if(bitmask_input == \"comp\"):\n","    for data in test_dataloader:\n","      lab_comp = data[\"label_comp\"]\n","      lab_sent = data[\"label_sent\"]\n","      lab_sev = data[\"label_sev\"]\n","      lab_emo = data[\"label_emo\"]\n","      output = complaint_model.forward(input_ids=data[\"input_ids\"], attention_mask=data[\"attention_mask\"])\n","      loss = criterion(output, lab_comp)\n","\n","      _, pred_comp = torch.max(output.data,1)\n","\n","      comp_accuracy.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_precision.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_f1.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_recall.update(pred_comp.cpu(), lab_comp.cpu())\n","\n","    comp_test_accuracy = comp_accuracy.compute()\n","    comp_test_precision = comp_precision.compute()\n","    comp_test_f1 = comp_f1.compute()\n","    comp_test_recall = comp_recall.compute()\n","    print(f\"comp_test_accuracy : {comp_test_accuracy}\")\n","    print(f\"comp_test_precision : {comp_test_precision}\")\n","    print(f\"comp_test_f1 : {comp_test_f1}\")\n","    print(f\"comp_test_recall : {comp_test_recall}\")\n","  \n","  elif(bitmask_input == \"sent\"):\n","    for data in test_dataloader:\n","      lab_comp = data[\"label_comp\"]\n","      lab_sent = data[\"label_sent\"]\n","      lab_sev = data[\"label_sev\"]\n","      lab_emo = data[\"label_emo\"]\n","      out = complaint_model.forward(input_ids=data[\"input_ids\"], attention_mask=data[\"attention_mask\"])\n","      loss = criterion(out, lab_sent)\n","\n","      _, pred_sent = torch.max(out2.data,1)\n","\n","      sent_accuracy.update(pred_sent.cpu(), lab_sent.cpu())\n","      sent_precision.update(pred_sent.cpu(), lab_sent.cpu())\n","      sent_f1.update(pred_sent.cpu(), lab_sent.cpu())\n","      sent_recall.update(pred_sent.cpu(), lab_sent.cpu())\n","\n","    sent_test_accuracy = sent_accuracy.compute()\n","    sent_test_precision = sent_precision.compute()\n","    sent_test_f1 = sent_f1.compute()\n","    sent_test_recall = sent_recall.compute()\n","\n","    print(f\"sent_test_accuracy : {sent_test_accuracy}\")\n","    print(f\"sent_test_precision : {sent_test_precision}\")\n","    print(f\"sent_test_f1 : {sent_test_f1}\")\n","    print(f\"sent_test_recall : {sent_test_recall}\")\n","\n","  elif(bitmask_input == \"emo\"):\n","    for data in test_dataloader:\n","      lab_comp = data[\"label_comp\"]\n","      lab_sent = data[\"label_sent\"]\n","      lab_sev = data[\"label_sev\"]\n","      lab_emo = data[\"label_emo\"]\n","      out = complaint_model.forward(input_ids=data[\"input_ids\"], attention_mask=data[\"attention_mask\"])\n","      loss = criterion(out, lab_emo)\n","\n","      _, pred_emo = torch.max(out3.data,1)\n","\n","      emo_accuracy.update(pred_emo.cpu(), lab_emo.cpu())\n","      emo_precision.update(pred_emo.cpu(), lab_emo.cpu())\n","      emo_f1.update(pred_emo.cpu(), lab_emo.cpu())\n","      emo_recall.update(pred_emo.cpu(), lab_emo.cpu())\n","  \n","    emo_test_accuracy = emo_accuracy.compute()\n","    emo_test_precision = emo_precision.compute()\n","    emo_test_f1 = emo_f1.compute()\n","    emo_test_recall = emo_recall.compute()\n","    \n","    print(f\"emo_test_accuracy : {emo_test_accuracy}\")\n","    print(f\"emo_test_precision : {emo_test_precision}\")\n","    print(f\"emo_test_f1 : {emo_test_f1}\")\n","    print(f\"emo_test_recall : {emo_test_recall}\")\n","\n","  elif(bitmask_input == \"comp+sent\"):\n","    for data in test_dataloader:\n","      lab_comp = data[\"label_comp\"]\n","      lab_sent = data[\"label_sent\"]\n","      lab_sev = data[\"label_sev\"]\n","      lab_emo = data[\"label_emo\"]\n","      out1, out2 = complaint_model.forward(input_ids=data[\"input_ids\"], attention_mask=data[\"attention_mask\"])\n","      loss = criterion(out1, lab_comp) + criterion(out2, lab_sent)\n","\n","      _, pred_comp = torch.max(out1.data,1)\n","      _, pred_sent = torch.max(out2.data,1)\n","\n","      comp_accuracy.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_precision.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_f1.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_recall.update(pred_comp.cpu(), lab_comp.cpu())\n","\n","      sent_accuracy.update(pred_sent.cpu(), lab_sent.cpu())\n","      sent_precision.update(pred_sent.cpu(), lab_sent.cpu())\n","      sent_f1.update(pred_sent.cpu(), lab_sent.cpu())\n","      sent_recall.update(pred_sent.cpu(), lab_sent.cpu())\n","      comp_test_accuracy = comp_accuracy.compute()\n","\n","    comp_test_accuracy = comp_accuracy.compute()\n","    comp_test_precision = comp_precision.compute()\n","    comp_test_f1 = comp_f1.compute()\n","    comp_test_recall = comp_recall.compute()\n","\n","    sent_test_accuracy = sent_accuracy.compute()\n","    sent_test_precision = sent_precision.compute()\n","    sent_test_f1 = sent_f1.compute()\n","    sent_test_recall = sent_recall.compute()\n","\n","    print(f\"comp_test_accuracy : {comp_test_accuracy}\")\n","    print(f\"comp_test_precision : {comp_test_precision}\")\n","    print(f\"comp_test_f1 : {comp_test_f1}\")\n","    print(f\"comp_test_recall : {comp_test_recall}\")\n","\n","    print(f\"sent_test_accuracy : {sent_test_accuracy}\")\n","    print(f\"sent_test_precision : {sent_test_precision}\")\n","    print(f\"sent_test_f1 : {sent_test_f1}\")\n","    print(f\"sent_test_recall : {sent_test_recall}\")\n","\n","\n","  elif(bitmask_input == \"comp+emo\"):\n","    for data in test_dataloader:\n","      lab_comp = data[\"label_comp\"]\n","      lab_sent = data[\"label_sent\"]\n","      lab_sev = data[\"label_sev\"]\n","      lab_emo = data[\"label_emo\"]\n","      out1, out2 = complaint_model.forward(input_ids=data[\"input_ids\"], attention_mask=data[\"attention_mask\"])\n","      loss = criterion(out1, lab_comp) + criterion(out2, lab_emo)\n","\n","      _, pred_comp = torch.max(out1.data,1)\n","      _, pred_emo = torch.max(out2.data,1)\n","\n","      comp_accuracy.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_precision.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_f1.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_recall.update(pred_comp.cpu(), lab_comp.cpu())\n","\n","      emo_accuracy.update(pred_emo.cpu(), lab_emo.cpu())\n","      emo_precision.update(pred_emo.cpu(), lab_emo.cpu())\n","      emo_f1.update(pred_emo.cpu(), lab_emo.cpu())\n","      emo_recall.update(pred_emo.cpu(), lab_emo.cpu())\n","\n","    comp_test_accuracy = comp_accuracy.compute()\n","    comp_test_precision = comp_precision.compute()\n","    comp_test_f1 = comp_f1.compute()\n","    comp_test_recall = comp_recall.compute()\n","\n","    emo_test_accuracy = emo_accuracy.compute()\n","    emo_test_precision = emo_precision.compute()\n","    emo_test_f1 = emo_f1.compute()\n","    emo_test_recall = emo_recall.compute()\n","\n","    print(f\"comp_test_accuracy : {comp_test_accuracy}\")\n","    print(f\"comp_test_precision : {comp_test_precision}\")\n","    print(f\"comp_test_f1 : {comp_test_f1}\")\n","    print(f\"comp_test_recall : {comp_test_recall}\")\n","\n","    print(f\"emo_test_accuracy : {emo_test_accuracy}\")\n","    print(f\"emo_test_precision : {emo_test_precision}\")\n","    print(f\"emo_test_f1 : {emo_test_f1}\")\n","    print(f\"emo_test_recall : {emo_test_recall}\")\n","\n","  elif(bitmask_input == \"comp+sev\"):\n","    for data in test_dataloader:\n","      lab_comp = data[\"label_comp\"]\n","      lab_sent = data[\"label_sent\"]\n","      lab_sev = data[\"label_sev\"]\n","      lab_emo = data[\"label_emo\"]\n","      out1, out2 = complaint_model.forward(input_ids=data[\"input_ids\"], attention_mask=data[\"attention_mask\"])\n","      loss = criterion(out1, lab_comp) + criterion(out2, lab_sev)\n","\n","      _, pred_comp = torch.max(out1.data,1)\n","      _, pred_sev = torch.max(out2.data,1)\n","\n","      comp_accuracy.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_precision.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_f1.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_recall.update(pred_comp.cpu(), lab_comp.cpu())\n","\n","      sev_accuracy.update(pred_sev.cpu(), lab_sev.cpu())\n","      sev_precision.update(pred_sev.cpu(), lab_sev.cpu())\n","      sev_f1.update(pred_sev.cpu(), lab_sev.cpu())\n","      sev_recall.update(pred_sev.cpu(), lab_sev.cpu())\n","\n","    comp_test_accuracy = comp_accuracy.compute()\n","    comp_test_precision = comp_precision.compute()\n","    comp_test_f1 = comp_f1.compute()\n","    comp_test_recall = comp_recall.compute()\n","\n","    sev_test_accuracy = sev_accuracy.compute()\n","    sev_test_precision = sev_precision.compute()\n","    sev_test_f1 = sev_f1.compute()\n","    sev_test_recall = sev_recall.compute()\n","\n","    print(f\"comp_test_accuracy : {comp_test_accuracy}\")\n","    print(f\"comp_test_precision : {comp_test_precision}\")\n","    print(f\"comp_test_f1 : {comp_test_f1}\")\n","    print(f\"comp_test_recall : {comp_test_recall}\")\n","\n","    print(f\"sev_test_accuracy : {sev_test_accuracy}\")\n","    print(f\"sev_test_precision : {sev_test_precision}\")\n","    print(f\"sev_test_f1 : {sev_test_f1}\")\n","    print(f\"sev_test_recall : {sev_test_recall}\")\n","\n","  elif(bitmask_input == \"comp+sent+emo\"):\n","    for data in test_dataloader:\n","      lab_comp = data[\"label_comp\"]\n","      lab_sent = data[\"label_sent\"]\n","      lab_sev = data[\"label_sev\"]\n","      lab_emo = data[\"label_emo\"]\n","      out1, out2, out3 = complaint_model.forward(input_ids=data[\"input_ids\"], attention_mask=data[\"attention_mask\"])\n","      loss = criterion(out1, lab_comp) + criterion(out2, lab_sent) + criterion(out3, lab_emo)\n","\n","      _, pred_comp = torch.max(out1.data,1)\n","      _, pred_sent = torch.max(out2.data,1)\n","      _, pred_emo = torch.max(out3.data,1)\n","\n","      comp_accuracy.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_precision.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_f1.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_recall.update(pred_comp.cpu(), lab_comp.cpu())\n","\n","      sent_accuracy.update(pred_sent.cpu(), lab_sent.cpu())\n","      sent_precision.update(pred_sent.cpu(), lab_sent.cpu())\n","      sent_f1.update(pred_sent.cpu(), lab_sent.cpu())\n","      sent_recall.update(pred_sent.cpu(), lab_sent.cpu())\n","\n","      emo_accuracy.update(pred_emo.cpu(), lab_emo.cpu())\n","      emo_precision.update(pred_emo.cpu(), lab_emo.cpu())\n","      emo_f1.update(pred_emo.cpu(), lab_emo.cpu())\n","      emo_recall.update(pred_emo.cpu(), lab_emo.cpu())\n","    \n","    comp_test_accuracy = comp_accuracy.compute()\n","    comp_test_precision = comp_precision.compute()\n","    comp_test_f1 = comp_f1.compute()\n","    comp_test_recall = comp_recall.compute()\n","\n","    emo_test_accuracy = emo_accuracy.compute()\n","    emo_test_precision = emo_precision.compute()\n","    emo_test_f1 = emo_f1.compute()\n","    emo_test_recall = emo_recall.compute()\n","\n","    sent_test_accuracy = sent_accuracy.compute()\n","    sent_test_precision = sent_precision.compute()\n","    sent_test_f1 = sent_f1.compute()\n","    sent_test_recall = sent_recall.compute()\n","\n","    print(f\"comp_test_accuracy : {comp_test_accuracy}\")\n","    print(f\"comp_test_precision : {comp_test_precision}\")\n","    print(f\"comp_test_f1 : {comp_test_f1}\")\n","    print(f\"comp_test_recall : {comp_test_recall}\")\n","\n","    print(f\"sent_test_accuracy : {sent_test_accuracy}\")\n","    print(f\"sent_test_precision : {sent_test_precision}\")\n","    print(f\"sent_test_f1 : {sent_test_f1}\")\n","    print(f\"sent_test_recall : {sent_test_recall}\")\n","\n","    print(f\"emo_test_accuracy : {emo_test_accuracy}\")\n","    print(f\"emo_test_precision : {emo_test_precision}\")\n","    print(f\"emo_test_f1 : {emo_test_f1}\")\n","    print(f\"emo_test_recall : {emo_test_recall}\")\n","\n","  elif(bitmask_input == \"comp+sent+sev\"):\n","    for data in test_dataloader:\n","      lab_comp = data[\"label_comp\"]\n","      lab_sent = data[\"label_sent\"]\n","      lab_sev = data[\"label_sev\"]\n","      lab_emo = data[\"label_emo\"]\n","      out1, out2, out3 = complaint_model.forward(input_ids=data[\"input_ids\"], attention_mask=data[\"attention_mask\"])\n","      loss = criterion(out1, lab_comp) + criterion(out2, lab_sent) + criterion(out3, lab_sev)\n","\n","      _, pred_comp = torch.max(out1.data,1)\n","      _, pred_sent = torch.max(out2.data,1)\n","      _, pred_sev = torch.max(out3.data,1)\n","\n","      comp_accuracy.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_precision.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_f1.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_recall.update(pred_comp.cpu(), lab_comp.cpu())\n","\n","      sent_accuracy.update(pred_sent.cpu(), lab_sent.cpu())\n","      sent_precision.update(pred_sent.cpu(), lab_sent.cpu())\n","      sent_f1.update(pred_sent.cpu(), lab_sent.cpu())\n","      sent_recall.update(pred_sent.cpu(), lab_sent.cpu())\n","\n","      sev_accuracy.update(pred_sev.cpu(), lab_sev.cpu())\n","      sev_precision.update(pred_sev.cpu(), lab_sev.cpu())\n","      sev_f1.update(pred_sev.cpu(), lab_sev.cpu())\n","      sev_recall.update(pred_sev.cpu(), lab_sev.cpu())\n","\n","    comp_test_accuracy = comp_accuracy.compute()\n","    comp_test_precision = comp_precision.compute()\n","    comp_test_f1 = comp_f1.compute()\n","    comp_test_recall = comp_recall.compute()\n","\n","    sev_test_accuracy = sev_accuracy.compute()\n","    sev_test_precision = sev_precision.compute()\n","    sev_test_f1 = sev_f1.compute()\n","    sev_test_recall = sev_recall.compute()\n","\n","    sent_test_accuracy = sent_accuracy.compute()\n","    sent_test_precision = sent_precision.compute()\n","    sent_test_f1 = sent_f1.compute()\n","    sent_test_recall = sent_recall.compute()\n","\n","    print(f\"comp_test_accuracy : {comp_test_accuracy}\")\n","    print(f\"comp_test_precision : {comp_test_precision}\")\n","    print(f\"comp_test_f1 : {comp_test_f1}\")\n","    print(f\"comp_test_recall : {comp_test_recall}\")\n","\n","    print(f\"sent_test_accuracy : {sent_test_accuracy}\")\n","    print(f\"sent_test_precision : {sent_test_precision}\")\n","    print(f\"sent_test_f1 : {sent_test_f1}\")\n","    print(f\"sent_test_recall : {sent_test_recall}\")\n","\n","    print(f\"sev_test_accuracy : {sev_test_accuracy}\")\n","    print(f\"sev_test_precision : {sev_test_precision}\")\n","    print(f\"sev_test_f1 : {sev_test_f1}\")\n","    print(f\"sev_test_recall : {sev_test_recall}\")\n","\n","  elif(bitmask_input == \"comp+emo+sev\"):\n","    for data in test_dataloader:\n","      lab_comp = data[\"label_comp\"]\n","      lab_sent = data[\"label_sent\"]\n","      lab_sev = data[\"label_sev\"]\n","      lab_emo = data[\"label_emo\"]\n","      out1, out2, out3 = complaint_model.forward(input_ids=data[\"input_ids\"], attention_mask=data[\"attention_mask\"])\n","      loss = criterion(out1, lab_comp) + criterion(out2, lab_emo) + criterion(out3, lab_sev)\n","\n","      _, pred_comp = torch.max(out1.data,1)\n","      _, pred_emo = torch.max(out2.data,1)\n","      _, pred_sev = torch.max(out3.data,1)\n","\n","      comp_accuracy.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_precision.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_f1.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_recall.update(pred_comp.cpu(), lab_comp.cpu())\n","\n","      emo_accuracy.update(pred_emo.cpu(), lab_emo.cpu())\n","      emo_precision.update(pred_emo.cpu(), lab_emo.cpu())\n","      emo_f1.update(pred_emo.cpu(), lab_emo.cpu())\n","      emo_recall.update(pred_emo.cpu(), lab_emo.cpu())\n","\n","      sev_accuracy.update(pred_sev.cpu(), lab_sev.cpu())\n","      sev_precision.update(pred_sev.cpu(), lab_sev.cpu())\n","      sev_f1.update(pred_sev.cpu(), lab_sev.cpu())\n","      sev_recall.update(pred_sev.cpu(), lab_sev.cpu())\n","    \n","    comp_test_accuracy = comp_accuracy.compute()\n","    comp_test_precision = comp_precision.compute()\n","    comp_test_f1 = comp_f1.compute()\n","    comp_test_recall = comp_recall.compute()\n","\n","    sev_test_accuracy = sev_accuracy.compute()\n","    sev_test_precision = sev_precision.compute()\n","    sev_test_f1 = sev_f1.compute()\n","    sev_test_recall = sev_recall.compute()\n","\n","    emo_test_accuracy = emo_accuracy.compute()\n","    emo_test_precision = emo_precision.compute()\n","    emo_test_f1 = emo_f1.compute()\n","    emo_test_recall = emo_recall.compute()\n","\n","    print(f\"comp_test_accuracy : {comp_test_accuracy}\")\n","    print(f\"comp_test_precision : {comp_test_precision}\")\n","    print(f\"comp_test_f1 : {comp_test_f1}\")\n","    print(f\"comp_test_recall : {comp_test_recall}\")\n","\n","    print(f\"emo_test_accuracy : {emo_test_accuracy}\")\n","    print(f\"emo_test_precision : {emo_test_precision}\")\n","    print(f\"emo_test_f1 : {emo_test_f1}\")\n","    print(f\"emo_test_recall : {emo_test_recall}\")\n","\n","    print(f\"sev_test_accuracy : {sev_test_accuracy}\")\n","    print(f\"sev_test_precision : {sev_test_precision}\")\n","    print(f\"sev_test_f1 : {sev_test_f1}\")\n","    print(f\"sev_test_recall : {sev_test_recall}\")\n","\n","  elif(bitmask_input == \"comp+emo+sev+sent\"):\n","    for data in test_dataloader:\n","      lab_comp = data[\"label_comp\"]\n","      lab_sent = data[\"label_sent\"]\n","      lab_sev = data[\"label_sev\"]\n","      lab_emo = data[\"label_emo\"]\n","      out1, out2, out3, out4 = complaint_model.forward(input_ids=data[\"input_ids\"], attention_mask=data[\"attention_mask\"])\n","      loss = criterion(out1, lab_comp) + criterion(out2, lab_emo) + criterion(out3, lab_sev) + criterion(out4, lab_sent)\n","\n","      _, pred_comp = torch.max(out1.data,1)\n","      _, pred_emo = torch.max(out2.data,1)\n","      _, pred_sev = torch.max(out3.data,1)\n","      _, pred_sent = torch.max(out4.data,1)\n","\n","      comp_accuracy.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_precision.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_f1.update(pred_comp.cpu(), lab_comp.cpu())\n","      comp_recall.update(pred_comp.cpu(), lab_comp.cpu())\n","\n","      emo_accuracy.update(pred_emo.cpu(), lab_emo.cpu())\n","      emo_precision.update(pred_emo.cpu(), lab_emo.cpu())\n","      emo_f1.update(pred_emo.cpu(), lab_emo.cpu())\n","      emo_recall.update(pred_emo.cpu(), lab_emo.cpu())\n","\n","      sev_accuracy.update(pred_sev.cpu(), lab_sev.cpu())\n","      sev_precision.update(pred_sev.cpu(), lab_sev.cpu())\n","      sev_f1.update(pred_sev.cpu(), lab_sev.cpu())\n","      sev_recall.update(pred_sev.cpu(), lab_sev.cpu())\n","\n","      sent_accuracy.update(pred_sent.cpu(), lab_sent.cpu())\n","      sent_precision.update(pred_sent.cpu(), lab_sent.cpu())\n","      sent_f1.update(pred_sent.cpu(), lab_sent.cpu())\n","      sent_recall.update(pred_sent.cpu(), lab_sent.cpu())\n","    comp_test_accuracy = comp_accuracy.compute()\n","    comp_test_precision = comp_precision.compute()\n","    comp_test_f1 = comp_f1.compute()\n","    comp_test_recall = comp_recall.compute()\n","\n","    sev_test_accuracy = sev_accuracy.compute()\n","    sev_test_precision = sev_precision.compute()\n","    sev_test_f1 = sev_f1.compute()\n","    sev_test_recall = sev_recall.compute()\n","\n","    emo_test_accuracy = emo_accuracy.compute()\n","    emo_test_precision = emo_precision.compute()\n","    emo_test_f1 = emo_f1.compute()\n","    emo_test_recall = emo_recall.compute()\n","\n","    sent_test_accuracy = sent_accuracy.compute()\n","    sent_test_precision = sent_precision.compute()\n","    sent_test_f1 = sent_f1.compute()\n","    sent_test_recall = sent_recall.compute()\n","    \n","    print(f\"comp_test_accuracy : {comp_test_accuracy}\")\n","    print(f\"comp_test_precision : {comp_test_precision}\")\n","    print(f\"comp_test_f1 : {comp_test_f1}\")\n","    print(f\"comp_test_recall : {comp_test_recall}\")\n","\n","    print(f\"sent_test_accuracy : {sent_test_accuracy}\")\n","    print(f\"sent_test_precision : {sent_test_precision}\")\n","    print(f\"sent_test_f1 : {sent_test_f1}\")\n","    print(f\"sent_test_recall : {sent_test_recall}\")\n","\n","    print(f\"emo_test_accuracy : {emo_test_accuracy}\")\n","    print(f\"emo_test_precision : {emo_test_precision}\")\n","    print(f\"emo_test_f1 : {emo_test_f1}\")\n","    print(f\"emo_test_recall : {emo_test_recall}\")\n","\n","    print(f\"sev_test_accuracy : {sev_test_accuracy}\")\n","    print(f\"sev_test_precision : {sev_test_precision}\")\n","    print(f\"sev_test_f1 : {sev_test_f1}\")\n","    print(f\"sev_test_recall : {sev_test_recall}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"016e3578fe7343d28d130f02b415e449":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9afd2a789d1640f591f0935d3609d012","placeholder":"​","style":"IPY_MODEL_aace6f9220114616a41d537ffc3153da","value":"Downloading (…)okenizer_config.json: 100%"}},"051534d1b4274ec381e37b9c178fbe5f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b5b26753f7f472d8e512a8f10f2abca","placeholder":"​","style":"IPY_MODEL_ca30e66617d94c518e9aa5bc132e3467","value":"Downloading pytorch_model.bin: 100%"}},"0b5c58df461545a093c2feb987367fdf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8344248e0c0145e7819fa213e81a3e69","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_251ce3f8b49b4e238b98c183a3792569","value":440473133}},"0d87f2f5148e4ebe8a51b4d8094e2583":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f5f8909eb5e420898d1894171383400":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8eb3550559df47c1a32067a0cfc847a4","placeholder":"​","style":"IPY_MODEL_0d87f2f5148e4ebe8a51b4d8094e2583","value":" 466k/466k [00:00&lt;00:00, 4.52MB/s]"}},"1619eede37d64d36996ef1044ede5bf2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e31ae7bcb67e47fbb6d8a5adc489c4d5","placeholder":"​","style":"IPY_MODEL_e198798a62284569b1035f4d77d800b3","value":" 440M/440M [00:02&lt;00:00, 218MB/s]"}},"1b201b5ca7b24378bfc6f7bb64e1bbf0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fceb4737505411bb57609076cc564c4","placeholder":"​","style":"IPY_MODEL_1cae7f493e004b1696998c3a755b144e","value":"Downloading (…)/main/tokenizer.json: 100%"}},"1b5b26753f7f472d8e512a8f10f2abca":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cae7f493e004b1696998c3a755b144e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1fe3b26fd70d40b6a7958d5b5cc3f486":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_72f460aaf8054155a659b76267a0b617","IPY_MODEL_9469337726894692a7dcebc8fd95191d","IPY_MODEL_c3e32ec92ac1402e8065d621d7fca871"],"layout":"IPY_MODEL_4b98ba4a541b4008b46a4d577144446f"}},"251ce3f8b49b4e238b98c183a3792569":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2b890c5a78c24dc29a3d979a3d86b8cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa4d55127d5242649d8e6fab90808dbc","placeholder":"​","style":"IPY_MODEL_897581ef390747819af90caecebccc76","value":" 28.0/28.0 [00:00&lt;00:00, 1.48kB/s]"}},"2e048e482e9345808f348ce8bfd9f7a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_051534d1b4274ec381e37b9c178fbe5f","IPY_MODEL_0b5c58df461545a093c2feb987367fdf","IPY_MODEL_1619eede37d64d36996ef1044ede5bf2"],"layout":"IPY_MODEL_e87d512953414cf8aa96850432fb46f6"}},"30cf8e8dac834af1b9844d67485ab2fc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32ed8b1c877d4ed39362bb3464d9eff7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1b201b5ca7b24378bfc6f7bb64e1bbf0","IPY_MODEL_84c9ab5130a441ecacd8a653bcf03ed4","IPY_MODEL_0f5f8909eb5e420898d1894171383400"],"layout":"IPY_MODEL_7ef73da28f6047b08e988a58e8da3b3b"}},"340f1d89c22043629093b13ceb8a8fc3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_73d2196e952440d3943d2be21dfd24d4","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_55ab4f485b7f414ebea7d51b56b5bd40","value":570}},"36dc54fc4c494e8fbf52773e3b87fcf1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a65addb4f9f479083435861dca8d776":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b98ba4a541b4008b46a4d577144446f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"554c916927b44e6082bb550b7ed0c4bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"55ab4f485b7f414ebea7d51b56b5bd40":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5bbbce3e7cea483c90fe3982f3f8b7b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6bce2f771fb64bd0bca113f0a955d24b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6bfc8b04ffa84568aeef1c8da7bb9730":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fceb4737505411bb57609076cc564c4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70417d99b6d849cf8a7263f92a059c28":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"715c541430ef46f594a1dce7dbd1b563":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72f460aaf8054155a659b76267a0b617":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a65addb4f9f479083435861dca8d776","placeholder":"​","style":"IPY_MODEL_554c916927b44e6082bb550b7ed0c4bb","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"737bd17f81614d22ac623b3cd13cd679":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bfc8b04ffa84568aeef1c8da7bb9730","placeholder":"​","style":"IPY_MODEL_be1bbea3a3934668b2bb0baf041acdf7","value":"Downloading (…)lve/main/config.json: 100%"}},"73d2196e952440d3943d2be21dfd24d4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ef73da28f6047b08e988a58e8da3b3b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8086c58f1e894b808eac8d66eaa0653a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8344248e0c0145e7819fa213e81a3e69":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84c9ab5130a441ecacd8a653bcf03ed4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_30cf8e8dac834af1b9844d67485ab2fc","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_36dc54fc4c494e8fbf52773e3b87fcf1","value":466062}},"897581ef390747819af90caecebccc76":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8eb3550559df47c1a32067a0cfc847a4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"908b634aa0e944b49deae3b8896b0ec2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9469337726894692a7dcebc8fd95191d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff42a75457384ebe90229ca171f24a59","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f5973f4a373a45e6b9c0273de38291f1","value":231508}},"9537617131c0451e907749bdab06f521":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9afd2a789d1640f591f0935d3609d012":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aace6f9220114616a41d537ffc3153da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2fc7e9d13364523a433c27ee89c14db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_908b634aa0e944b49deae3b8896b0ec2","placeholder":"​","style":"IPY_MODEL_5bbbce3e7cea483c90fe3982f3f8b7b0","value":" 570/570 [00:00&lt;00:00, 34.0kB/s]"}},"be1bbea3a3934668b2bb0baf041acdf7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3e32ec92ac1402e8065d621d7fca871":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8086c58f1e894b808eac8d66eaa0653a","placeholder":"​","style":"IPY_MODEL_6bce2f771fb64bd0bca113f0a955d24b","value":" 232k/232k [00:00&lt;00:00, 4.56MB/s]"}},"ca30e66617d94c518e9aa5bc132e3467":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf063c23004c4d56b9aa0e996fd787c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_737bd17f81614d22ac623b3cd13cd679","IPY_MODEL_340f1d89c22043629093b13ceb8a8fc3","IPY_MODEL_b2fc7e9d13364523a433c27ee89c14db"],"layout":"IPY_MODEL_9537617131c0451e907749bdab06f521"}},"d0f203efd4b143eea2d3c133a16c4598":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_715c541430ef46f594a1dce7dbd1b563","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f0b373e04199497ebf8618ce16a11b11","value":28}},"e198798a62284569b1035f4d77d800b3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e31ae7bcb67e47fbb6d8a5adc489c4d5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e87d512953414cf8aa96850432fb46f6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e97aee74212243359d226fcd61d258e3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_016e3578fe7343d28d130f02b415e449","IPY_MODEL_d0f203efd4b143eea2d3c133a16c4598","IPY_MODEL_2b890c5a78c24dc29a3d979a3d86b8cc"],"layout":"IPY_MODEL_70417d99b6d849cf8a7263f92a059c28"}},"f0b373e04199497ebf8618ce16a11b11":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f5973f4a373a45e6b9c0273de38291f1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fa4d55127d5242649d8e6fab90808dbc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff42a75457384ebe90229ca171f24a59":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
